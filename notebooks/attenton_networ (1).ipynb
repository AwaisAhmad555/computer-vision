{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"attenton_networ (1).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import  scipy.io as sio\n","import os\n","import cv2 as cv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import glob\n","import random\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.utils import to_categorical\n"],"metadata":{"id":"HAxneYVOWDvo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4hhNsSaWI1H","executionInfo":{"status":"ok","timestamp":1658002378500,"user_tz":420,"elapsed":2760,"user":{"displayName":"NextTech Skills","userId":"01791669505436739759"}},"outputId":"32995b1b-0a0c-4ea0-bcb9-cd077fd53f2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# attentonlayer"],"metadata":{"id":"zn5wnf6C1Cwl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gPjTShUU02NO"},"outputs":[],"source":["\n","from keras.layers import Layer\n","import keras.backend as K\n","\n","\n","class SpatialAttention(Layer):\n","    def __init__(self,\n","                 **kwargs):\n","        super(SpatialAttention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.kernel = self.add_weight(shape=[input_shape[-2], 1],\n","                                 name='kernel',\n","                                 initializer='ones',\n","                                 trainable=True)\n","\n","        self.bias = self.add_weight(shape=[input_shape[-2]],\n","                                 name='bias',\n","                                 initializer='zeros',\n","                                 trainable=True)\n","\n","    def call(self, inputs, training=None):\n","\n","        input_shape = K.int_shape(inputs)\n","        mid = input_shape[-2] // 2\n","\n","        coe = K.l2_normalize(K.batch_dot(inputs, K.permute_dimensions(inputs, pattern=(0, 2, 1))), axis=-1)\n","        coe0 = K.expand_dims(coe[:, mid, :], axis=-1) * self.kernel\n","        w = K.batch_dot(coe, coe0) + K.expand_dims(self.bias, axis=-1)\n","        outputs = K.softmax(w, axis=-2) * inputs\n","\n","        return outputs\n","\n","    def get_config(self):\n","        config = {}\n","        base_config = super(SpatialAttention, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def compute_output_shape(self, input_shape):\n","        return tuple(input_shape)"]},{"cell_type":"markdown","source":["# second pool layer"],"metadata":{"id":"dYJec1Iz1KtU"}},{"cell_type":"code","source":["\n","from keras.layers import Layer\n","import keras.backend as K\n","\n","\n","class SecondOrderPooling(Layer):\n","    def __init__(self,\n","                 **kwargs):\n","        super(SecondOrderPooling, self).__init__(**kwargs)\n","\n","    def call(self, inputs, training=None):\n","        input_shape = K.int_shape(inputs)\n","\n","        outputs = K.batch_dot(K.permute_dimensions(inputs, pattern=(0, 2, 1)), inputs, axes=[2, 1])\n","        outputs = K.reshape(outputs, [-1, input_shape[2] * input_shape[2]])\n","\n","        return outputs\n","\n","    def get_config(self):\n","        config = {}\n","        base_config = super(SecondOrderPooling, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def compute_output_shape(self, input_shape):\n","        if len(input_shape) == 4:\n","            output_shape = list([None, input_shape[1], input_shape[3] * input_shape[3]])\n","        else:\n","            output_shape = list([None, input_shape[2] * input_shape[2]])\n","        return tuple(output_shape)"],"metadata":{"id":"_o26XS3k1U3R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# model"],"metadata":{"id":"NPx4xCYB1YeA"}},{"cell_type":"code","source":["from keras.layers import Input\n","from keras.layers import Flatten, Dense, Reshape,BatchNormalization,Dropout,Lambda\n","import math\n","from tensorflow.keras.models import Model\n","from keras.models import Model\n","from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda, Activation\n","from keras.layers import ZeroPadding2D, ZeroPadding1D\n","from keras import backend as K"],"metadata":{"id":"WQUX3K20104S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy\n","def Symmetry(a, rtol=1e-05):\n","    return numpy.allclose( a.T, rtol=rtol)"],"metadata":{"id":"DyA-29P93bBt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def aspn(img_rows, img_cols, num_PC, nb_classes):\n","    CNNInput = Input(shape=(img_rows, img_cols, num_PC), name='i0')\n","    print(CNNInput.shape)\n","    F = Reshape([img_rows * img_cols, num_PC])(CNNInput)\n","    F = BatchNormalization()(F)\n","    F = Dropout(rate=0.5)(F)\n","\n","    F = Lambda(lambda x: K.l2_normalize(x, axis=-1), name='f2')(F)\n","    F = SpatialAttention(name='f3')(F)\n","    F = SecondOrderPooling(name='feature1')(F)\n","    F = Lambda(lambda x: K.l2_normalize(x, axis=-1), name='feature2')(F)\n","    print(F.shape)\n","    R1 = Reshape((32,32,1))(F)\n","    print(R1.shape)\n","    upsamp_1 = UpSampling2D(size=(2,2))(R1)\n","    print(upsamp_1.shape)\n","    upsamp_2 = UpSampling2D(size=(2,2))(upsamp_1)\n","    print(upsamp_2.shape)\n","    upsamp_3 = UpSampling2D(size=(2,2))(upsamp_2)\n","    print(upsamp_3.shape)\n","    #upsamp_1 = ZeroPadding2D(padding=((2,2),(2,2)))(upsamp_3)\n","    #upsamp_2= ZeroPadding2D(padding=((2,2),(2,2)))(upsamp_1)\n","    #upsamp_3 = ZeroPadding2D(padding=((2,2),(2,2)))(upsamp_2)\n","    #upsamp_4= ZeroPadding2D(padding=((2,2),(2,2)))(upsamp_3)\n","    \n","    #print(upsamp_1.shape)\n","    conv = Conv2D(41, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(upsamp_3)\n","    print(conv.shape)\n","    outputs = Conv2D(nb_classes, (1, 1), activation='softmax')(conv)\n","     \n","   #F = Dense(nb_classes, activation='softmax', name='classifier', kernel_initializer ='he_normal')(F)\n","    \n","    model = Model(inputs=[CNNInput], outputs=outputs)\n","    model.summary()\n","    return model\n","\n","#aspn(256 ,256, 32, 41)\n"],"metadata":{"id":"x4GzyphZ1a9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_img(img_dir, img_list):\n","   images = []\n","   ground_truth_list = []\n","\n","   for i, image_name in enumerate(img_list):\n","\n","      if (image_name.split('.')[1] == 'npz'):\n","         image_file_path = os.path.join(img_dir, image_name)\n","\n","         MRI_image = np.load(image_file_path)\n","\n","         image = MRI_image['a']\n","\n","         # image = np.transpose(MRI_image['a'],(1,2,0))          No need to transpose\n","\n","         # images.append(image[56:184, 56:184])\n","\n","         images.append(image)\n","\n","         image_ground_truth = MRI_image['b'].astype(np.uint8)\n","\n","         # image_ground_truth = image_ground_truth[56:184, 56:184]\n","\n","         # n_classes = len(np.unique(image_ground_truth))           not all images have all 10 classes labels so only going to take 10 as total classes\n","         #print(np.unique(image_ground_truth))\n","         n_classes = 41\n","\n","         image_ground_truth = to_categorical(image_ground_truth, num_classes=n_classes)\n","\n","         ground_truth_list.append(image_ground_truth)\n","\n","         pass\n","\n","      pass\n","\n","   images = np.array(images)\n","   ground_truth_list = np.array(ground_truth_list)\n","\n","   return (images, ground_truth_list)\n","\n","\n","def imageLoader(img_dir, img_list, batch_size):\n","   L = len(img_list)\n","\n","   # keras needs the generator infinite, so we will use while true\n","   while True:\n","\n","      batch_start = 0\n","      batch_end = batch_size\n","\n","      while batch_start < L:\n","         limit = min(batch_end, L)\n","\n","         X, Y = load_img(img_dir, img_list[batch_start:limit])\n","\n","         yield (X, Y)  # a tuple with two numpy arrays with batch_size samples\n","\n","         batch_start += batch_size\n","         batch_end += batch_size\n"],"metadata":{"id":"2vu-BKp1VGjq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","def load_npz_files(npz_path):\n","      train_files = []\n","      train_files.append([name for name in glob.glob(npz_path + \"*.npz\")])\n","      train_files = train_files[0]\n","      random.shuffle(train_files)\n","      return train_files"],"metadata":{"id":"0Me0UCMkVaac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import FactorAnalysis\n","def applyFA(X, numComponents=32):\n","    print('FA starts')\n","    newX = np.reshape(X, (-1, X.shape[2]))\n","    fa = FactorAnalysis(n_components=numComponents, random_state=0)\n","    newX = fa.fit_transform(newX)\n","    newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))\n","    return newX, fa\n","\n"],"metadata":{"id":"jHhcZjltdH1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","if __name__ == \"__main__\":\n","   #split data \n","   selected_fles=[]\n","   npz_files_folder_path = '/content/drive/MyDrive/Aiman/HSI/ip+salinas+paviau/'\n","   all_fles =os.listdir(npz_files_folder_path)\n","   print('all fles' ,len(all_fles))\n","   selected_fles.append(all_fles[0:22])\n","   print('selected fles' ,len(selected_fles))\n","   X_trn, X_tst= train_test_split(all_fles, test_size=0.2, random_state=42)\n","   \n","   npz_files_folder_path = '/content/drive/MyDrive/Aiman/HSI/ip+salinas+paviau/'\n","   batch_size = 1\n","   train_images_dataGenerator = imageLoader(img_dir=npz_files_folder_path, img_list=X_trn,batch_size=batch_size)\n","  \n","   scaler = MinMaxScaler()\n","   # Update batch size to vary image batches\n","\n","   validation_batch_size = 1\n","\n","   training_npz_folder_path = '/content/drive/MyDrive/Aiman/HSI/ip+salinas+paviau/'\n","\n","   training_npz_files_list = X_trn[0:1]\n","   print(len(X_trn))\n","   print(len(X_tst))\n","   #print(training_npz_files_list)\n","   validation_npz_folder_path = '/content/drive/MyDrive/Aiman/HSI/ip+salinas+paviau/'\n","\n","   validation_npz_files_list = X_tst[2:3]\n","\n","   print(validation_npz_files_list)\n","\n","   train_images_dataGenerator = imageLoader(img_dir=training_npz_folder_path, img_list=training_npz_files_list, batch_size=batch_size)\n","\n","\n","\n","   validation_images_dataGenerator = imageLoader(img_dir=validation_npz_folder_path, img_list=validation_npz_files_list, batch_size=validation_batch_size)\n","  \n","\n","   X, Y = train_images_dataGenerator.__next__()\n","   print(validation_images_dataGenerator)\n","   print('X' ,X.shape)\n","   print('Y',Y.shape)\n","   X= np.squeeze(X ,axis=0)\n","   K = 32\n","   X,fa = applyFA(X, numComponents=K)\n","   print('X' ,X.shape)\n","   X= np.expand_dims(X ,axis=0)\n","   print('X' ,X.shape)\n","   steps_per_epoch = len(training_npz_files_list) // batch_size\n","   val_steps_per_epoch = len(validation_npz_files_list) // batch_size\n","\n","   print(steps_per_epoch)\n","\n","   model = aspn(img_rows=256, img_cols=256, num_PC=32, nb_classes=41)\n","\n","   # model.compile(optimizer = optim, loss=total_loss, metrics=metrics)\n","\n","   model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","   print(model.summary())\n","\n","   print(model.input_shape)\n","   print(model.output_shape)\n","\n","   history = model.fit(train_images_dataGenerator,\n","                       steps_per_epoch=steps_per_epoch,\n","                       epochs=500,\n","                       verbose=1,\n","                       validation_data=validation_images_dataGenerator,\n","                       validation_steps=val_steps_per_epoch,\n","                       \n","                       )\n","\n","   model.save('/content/drive/MyDrive/Aiman/HSI/dense_unet.hdf5')\n","   loss = history.history['loss']\n","   val_loss = history.history['val_loss']\n","   epochs = range(1, len(loss) + 1)\n","   plt.plot(epochs, loss, 'y', label='Training loss')\n","   plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","   plt.title('Training and validation loss')\n","   plt.xlabel('Epochs')\n","   plt.ylabel('Loss')\n","   plt.legend()\n","   plt.savefig(\"loss_curve.pdf\")\n","   plt.savefig(\"loss_curve.jpg\")\n","   plt.show()\n","\n","   acc = history.history['accuracy']\n","   val_acc = history.history['val_accuracy']\n","\n","   plt.plot(epochs, acc, 'y', label='Training accuracy')\n","   plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n","\n","   plt.title('Training and validation accuracy')\n","   plt.xlabel('Epochs')\n","   plt.ylabel('Accuracy')\n","   plt.legend()\n","   plt.savefig(\"acc_curve.pdf\")\n","   plt.savefig(\"acc_curve.jpg\")\n","   plt.show()\n","\n","   from keras.models import load_model\n","\n","npz_files_folder_path = '/content/drive/MyDrive/Aiman/HSI/ip+salinas+paviau/'\n","hyperspectral_npz_files = load_npz_files(npz_files_folder_path)\n","batch_size = 4\n","train_images_dataGenerator = imageLoader(img_dir=npz_files_folder_path, img_list=hyperspectral_npz_files,\n","                                         batch_size=batch_size)\n","\n","X, Y = train_images_dataGenerator.__next__()\n","\n","my_model = load_model('/content/drive/MyDrive/Aiman/HSI/dense_unet.hdf5')\n","\n","print(my_model)\n","\n","print()\n","\n","\"\"\"npz_files_folder_path = '/content/drive/MyDrive/pavia_Hyperspectral_images/'\n","\n","file_name_list = os.listdir(npz_files_folder_path)\n","\n","X, Y = load_img(img_dir=npz_files_folder_path,img_list=file_name_list)\"\"\"\n","\n","query_hyperspectral_image = X[0]\n","\n","print(query_hyperspectral_image.shape)\n","\n","test_img_input = np.expand_dims(query_hyperspectral_image, axis=0)\n","\n","print(test_img_input.shape)\n","\n","# Prediction of UNET model\n","\n","test_prediction = my_model.predict(test_img_input)\n","\n","print(test_prediction.shape)\n","\n","# converting prediction to normal from to categorial i.e   (256 x 256 x 10) ==> (256 x 256 x 1)\n","\n","test_prediction_argmax = np.argmax(test_prediction, axis=3)[0, :, :]\n","\n","print(test_prediction_argmax.shape)\n","\n","prediction = test_prediction_argmax\n","\n","ground_truth = Y[0]\n","\n","print(\"-----\")\n","\n","# ground_truth = np.expand_dims(ground_truth, axis=2)\n","\n","print(ground_truth.shape)\n","\n","ground_truth = np.argmax(ground_truth, axis=2)\n","\n","plt.figure(figsize=(12, 8))\n","plt.subplot(231)\n","plt.title('Testing Image')\n","plt.imshow(query_hyperspectral_image[:, :, 1], cmap='gray')\n","plt.subplot(232)\n","plt.title('Testing Label')\n","plt.imshow(ground_truth)\n","plt.subplot(233)\n","plt.title('Prediction on test image')\n","plt.imshow(prediction[:, :])\n","plt.show()\n","\n","print(np.unique(prediction))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":727},"id":"y8QjuuXFVdla","executionInfo":{"status":"error","timestamp":1658002427434,"user_tz":420,"elapsed":18236,"user":{"displayName":"NextTech Skills","userId":"01791669505436739759"}},"outputId":"9bd6eacf-d812-4ab7-9be9-644b2b6d21a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["all fles 256\n","selected fles 1\n","204\n","52\n","['ip+salinas+paviau79.npz']\n","<generator object imageLoader at 0x7fe7d61bf150>\n","X (1, 256, 256, 150)\n","Y (1, 256, 256, 41)\n","FA starts\n","X (256, 256, 32)\n","X (1, 256, 256, 32)\n","1\n","(None, 256, 256, 32)\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-6c449b094ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m    \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maspn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_PC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m    \u001b[0;31m# model.compile(optimizer = optim, loss=total_loss, metrics=metrics)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-74-2c5c2c0e378d>\u001b[0m in \u001b[0;36maspn\u001b[0;34m(img_rows, img_cols, num_PC, nb_classes)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpatialAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSecondOrderPooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'feature1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-74-2c5c2c0e378d>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpatialAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSecondOrderPooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'feature1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling layer \"f2\" (type Lambda).\n\n'int' object has no attribute 'l2_normalize'\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 65536, 32), dtype=float32)\n  • mask=None\n  • training=False"]}]}]}